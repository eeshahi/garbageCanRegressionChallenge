---
title: "Garbage Can Regression Challenge"
format:
  html: default
execute:
  echo: false
  eval: true
---

# Garbage Can Regression Challenge

**Choose R or Python and delete the other code chunk.**


## Python Code

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm

# Data with known true relationships: Anxiety = Stress + 0.1 √ó Time
data = {
    'Stress': [0,0,0,1,1,1,2,2,2,8,8,8,12,12,12],
    'StressSurvey': [0,0,0,3,3,3,6,6,6,9,9,9,12,12,12],
    'Time': [0,1,1,1,1,1,2,2,2,2,2,2.1,2.2,2.2,2.2],
    'Anxiety': [0,0.1,0.1,1.1,1.1,1.1,2.2,2.2,2.2,8.2,8.2,8.21,12.22,12.22,12.22]
}

observDF = pd.DataFrame(data)
print(observDF)
```

## Your Analysis

Follow the challenge instructions from your course to complete your analysis.

### 75% Questions
### 1. Bivariate Regression: Anxiety on StressSurvey

```{python}
#| echo: false
# Bivariate regression of Anxiety on StressSurvey
X = observDF[['StressSurvey']]
y = observDF['Anxiety']

# Add constant for intercept
X_with_const = sm.add_constant(X)

# Fit the regression model
model = sm.OLS(y, X_with_const).fit()

# Display results
print("Bivariate Regression Results: Anxiety ~ StressSurvey")
print("=" * 50)
print(model.summary())

# Extract coefficients
intercept = model.params['const']
slope = model.params['StressSurvey']
r_squared = model.rsquared

print(f"\nEstimated Coefficients:")
print(f"Intercept: {intercept:.4f}")
print(f"Slope (StressSurvey): {slope:.4f}")
print(f"R-squared: {r_squared:.4f}")

# True relationship: Anxiety = Stress + 0.1 √ó Time
print(f"\nTrue Relationship: Anxiety = Stress + 0.1 √ó Time")
print("Note: The true model uses 'Stress' and 'Time', not 'StressSurvey'")
```

```{python}
#| echo: false
# Compare with true relationship
print("Comparison with True Relationship:")
print("=" * 40)

# Calculate true coefficients if we were regressing Anxiety on Stress
true_stress_coef = 1.0  # Coefficient of Stress in true model
true_intercept = 0.0    # No intercept in true model (approximately)

print(f"True coefficient for Stress: {true_stress_coef}")
print(f"Estimated coefficient for StressSurvey: {slope:.4f}")
print(f"Difference: {abs(slope - true_stress_coef):.4f}")

# Check if StressSurvey is a good proxy for Stress
correlation_stress_stressurvey = observDF['Stress'].corr(observDF['StressSurvey'])
print(f"\nCorrelation between Stress and StressSurvey: {correlation_stress_stressurvey:.4f}")

# Visualize the relationship
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Plot 1: Anxiety vs StressSurvey (what we're regressing)
ax1.scatter(observDF['StressSurvey'], observDF['Anxiety'], alpha=0.7)
ax1.plot(observDF['StressSurvey'], intercept + slope * observDF['StressSurvey'], 
         'r-', linewidth=2, label=f'Fitted line: Anxiety = {intercept:.3f} + {slope:.3f} √ó StressSurvey')
ax1.set_xlabel('StressSurvey')
ax1.set_ylabel('Anxiety')
ax1.set_title('Anxiety vs StressSurvey (Estimated)')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: Anxiety vs Stress (true relationship)
ax2.scatter(observDF['Stress'], observDF['Anxiety'], alpha=0.7)
ax2.plot(observDF['Stress'], true_intercept + true_stress_coef * observDF['Stress'], 
         'g-', linewidth=2, label='True relationship: Anxiety = Stress + 0.1 √ó Time')
ax2.set_xlabel('Stress')
ax2.set_ylabel('Anxiety')
ax2.set_title('Anxiety vs Stress (True Relationship)')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Interpretation of Results
It fits well but is wrong, because the true model is Anxiety = Stress + 0.1¬∑Time (intercept 0, no StressSurvey), so using the survey proxy and omitting Time biases the result.

### 2. Visualization of Bivariate Relationship

```{python}
#| echo: false
# Create detailed scatter plot with regression line
fig, ax = plt.subplots(figsize=(10, 6))

# Scatter plot
ax.scatter(observDF['StressSurvey'], observDF['Anxiety'], 
           alpha=0.7, s=100, color='steelblue', edgecolors='black', linewidth=1)

# Regression line
x_range = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 100)
y_pred = intercept + slope * x_range
ax.plot(x_range, y_pred, 'r-', linewidth=3, label=f'Regression Line: Anxiety = {intercept:.3f} + {slope:.3f} √ó StressSurvey')

# Add data point labels for clarity
for i, row in observDF.iterrows():
    ax.annotate(f'({row["StressSurvey"]}, {row["Anxiety"]:.2f})', 
                (row['StressSurvey'], row['Anxiety']),
                xytext=(5, 5), textcoords='offset points',
                fontsize=8, alpha=0.7)

# Formatting
ax.set_xlabel('StressSurvey', fontsize=12, fontweight='bold')
ax.set_ylabel('Anxiety', fontsize=12, fontweight='bold')
ax.set_title('Bivariate Relationship: Anxiety vs StressSurvey\nwith Regression Line', 
             fontsize=14, fontweight='bold', pad=20)
ax.legend(fontsize=11, loc='upper left')
ax.grid(True, alpha=0.3)
ax.set_facecolor('#f8f9fa')

# Add regression statistics as text
stats_text = f'R¬≤ = {r_squared:.4f}\nSlope = {slope:.4f}\nIntercept = {intercept:.4f}'
ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=10,
        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

plt.tight_layout()
plt.show()

# Additional analysis
print("Detailed Analysis of the Bivariate Relationship:")
print("=" * 50)
print(f"Number of observations: {len(observDF)}")
print(f"Range of StressSurvey: {observDF['StressSurvey'].min()} to {observDF['StressSurvey'].max()}")
print(f"Range of Anxiety: {observDF['Anxiety'].min():.2f} to {observDF['Anxiety'].max():.2f}")
print(f"Correlation coefficient: {observDF['StressSurvey'].corr(observDF['Anxiety']):.6f}")
print(f"Standard error of residuals: {np.sqrt(model.mse_resid):.4f}")
```

### Comments on Fit and Potential Issues
The scatter with the regression line shows a strong, roughly linear fit (points hug the line; high ùëÖ^2). However, the negative intercept is implausible, StressSurvey is only a proxy for true stress, and the model omits Time, so the slope can be biased; with the small sample, the largest StressSurvey point is also high-leverage.

### 3. Bivariate Regression Analysis with Time

```{python}
#| echo: false
# Bivariate regression of Anxiety on Time
X_time = observDF[['Time']]
y_time = observDF['Anxiety']

# Add constant for intercept
X_time_with_const = sm.add_constant(X_time)

# Fit the regression model
model_time = sm.OLS(y_time, X_time_with_const).fit()

# Display results
print("Bivariate Regression Results: Anxiety ~ Time")
print("=" * 50)
print(model_time.summary())

# Extract coefficients
intercept_time = model_time.params['const']
slope_time = model_time.params['Time']
r_squared_time = model_time.rsquared

print(f"\nEstimated Coefficients:")
print(f"Intercept: {intercept_time:.4f}")
print(f"Slope (Time): {slope_time:.4f}")
print(f"R-squared: {r_squared_time:.4f}")

# True relationship: Anxiety = Stress + 0.1 √ó Time
print(f"\nTrue Relationship: Anxiety = Stress + 0.1 √ó Time")
print("Note: The true coefficient for Time is 0.1")
```

```{python}
#| echo: false
# Compare with true relationship
print("Comparison with True Relationship:")
print("=" * 40)

# Calculate true coefficient for Time
true_time_coef = 0.1  # Coefficient of Time in true model
true_intercept_time = 0.0  # No intercept in true model (approximately)

print(f"True coefficient for Time: {true_time_coef}")
print(f"Estimated coefficient for Time: {slope_time:.4f}")
print(f"Difference: {abs(slope_time - true_time_coef):.4f}")

# Check the relationship between Time and the other variables
print(f"\nCorrelations:")
print(f"Time vs Stress: {observDF['Time'].corr(observDF['Stress']):.4f}")
print(f"Time vs Anxiety: {observDF['Time'].corr(observDF['Anxiety']):.4f}")

# Visualize the relationship
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Plot 1: Anxiety vs Time (what we're regressing)
ax1.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, s=100, color='darkgreen', edgecolors='black')
ax1.plot(observDF['Time'], intercept_time + slope_time * observDF['Time'], 
         'r-', linewidth=2, label=f'Fitted line: Anxiety = {intercept_time:.3f} + {slope_time:.3f} √ó Time')
ax1.set_xlabel('Time')
ax1.set_ylabel('Anxiety')
ax1.set_title('Anxiety vs Time (Estimated)')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: Show the true relationship components
ax2.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, s=100, color='darkgreen', edgecolors='black', label='Observed Anxiety')
# Show what Anxiety would be if it were only 0.1 √ó Time (true Time component)
true_time_component = 0.1 * observDF['Time']
ax2.scatter(observDF['Time'], true_time_component, alpha=0.7, s=60, color='orange', 
           marker='^', label='True Time component (0.1 √ó Time)')
ax2.plot(observDF['Time'], true_time_component, 'orange', linestyle='--', alpha=0.7)
ax2.set_xlabel('Time')
ax2.set_ylabel('Anxiety')
ax2.set_title('True Time Component vs Observed Anxiety')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Interpretation of Time Regression Results
Estimated coefficients (Anxiety ~ Time):
ùõΩ^0 =‚àí3.680 (intercept), ùõΩ^Time =5.341 (slope).

Comparison to the true relationship:
True model: ùõΩ0=0, ùõΩTime = 0.1
Your bivariate slope is vastly too large and the intercept is negative‚Äîthis mismatch occurs because Stress (a key driver) is omitted and is positively related to Time, causing omitted-variable bias.

### 4. Visualization of Bivariate Relationship

```{python}
#| echo: false
# Create detailed scatter plot with regression line for Time vs Anxiety
fig, ax = plt.subplots(figsize=(10, 6))

# Scatter plot
ax.scatter(observDF['Time'], observDF['Anxiety'], 
           alpha=0.7, s=120, color='darkgreen', edgecolors='black', linewidth=1.5)

# Regression line
x_range_time = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
y_pred_time = intercept_time + slope_time * x_range_time
ax.plot(x_range_time, y_pred_time, 'r-', linewidth=3, 
         label=f'Regression Line: Anxiety = {intercept_time:.3f} + {slope_time:.3f} √ó Time')

# Add data point labels for clarity
for i, row in observDF.iterrows():
    ax.annotate(f'({row["Time"]}, {row["Anxiety"]:.2f})', 
                (row['Time'], row['Anxiety']),
                xytext=(8, 8), textcoords='offset points',
                fontsize=9, alpha=0.8, fontweight='bold')

# Formatting
ax.set_xlabel('Time', fontsize=12, fontweight='bold')
ax.set_ylabel('Anxiety', fontsize=12, fontweight='bold')
ax.set_title('Bivariate Relationship: Anxiety vs Time\nwith Regression Line', 
             fontsize=14, fontweight='bold', pad=20)
ax.legend(fontsize=11, loc='upper left')
ax.grid(True, alpha=0.3)
ax.set_facecolor('#f8f9fa')

# Add regression statistics as text
stats_text_time = f'R¬≤ = {r_squared_time:.4f}\nSlope = {slope_time:.4f}\nIntercept = {intercept_time:.4f}\nTrue Time Coef = 0.1'
ax.text(0.02, 0.98, stats_text_time, transform=ax.transAxes, fontsize=10,
        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

# Add comparison to true relationship
true_line_y = 0.1 * x_range_time
ax.plot(x_range_time, true_line_y, 'orange', linestyle='--', linewidth=2, alpha=0.7,
         label='True Time Component (0.1 √ó Time)')

plt.tight_layout()
plt.show()

# Additional analysis
print("Detailed Analysis of Time-Anxiety Relationship:")
print("=" * 50)
print(f"Number of observations: {len(observDF)}")
print(f"Range of Time: {observDF['Time'].min()} to {observDF['Time'].max()}")
print(f"Range of Anxiety: {observDF['Anxiety'].min():.2f} to {observDF['Anxiety'].max():.2f}")
print(f"Correlation coefficient (Time-Anxiety): {observDF['Time'].corr(observDF['Anxiety']):.4f}")
print(f"Standard error of residuals: {np.sqrt(model_time.mse_resid):.4f}")
print(f"Estimated Time coefficient: {slope_time:.4f}")
print(f"True Time coefficient: 0.1")
print(f"Bias in Time coefficient: {slope_time - 0.1:.4f}")
```

### Comments on Time-Anxiety Fit and Potential Issues

 
