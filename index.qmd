---
title: "Garbage Can Regression Challenge"
format:
  html: default
execute:
  echo: false
  eval: true
---

# Garbage Can Regression Challenge

**Choose R or Python and delete the other code chunk.**


## Python Code

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm

# Data with known true relationships: Anxiety = Stress + 0.1 √ó Time
data = {
    'Stress': [0,0,0,1,1,1,2,2,2,8,8,8,12,12,12],
    'StressSurvey': [0,0,0,3,3,3,6,6,6,9,9,9,12,12,12],
    'Time': [0,1,1,1,1,1,2,2,2,2,2,2.1,2.2,2.2,2.2],
    'Anxiety': [0,0.1,0.1,1.1,1.1,1.1,2.2,2.2,2.2,8.2,8.2,8.21,12.22,12.22,12.22]
}

observDF = pd.DataFrame(data)
print(observDF)
```

## Your Analysis

Follow the challenge instructions from your course to complete your analysis.

### 75% Questions
### 1. Bivariate Regression: Anxiety on StressSurvey

```{python}
#| echo: false
# Bivariate regression of Anxiety on StressSurvey
X = observDF[['StressSurvey']]
y = observDF['Anxiety']

# Add constant for intercept
X_with_const = sm.add_constant(X)

# Fit the regression model
model = sm.OLS(y, X_with_const).fit()

# Display results
print("Bivariate Regression Results: Anxiety ~ StressSurvey")
print("=" * 50)
print(model.summary())

# Extract coefficients
intercept = model.params['const']
slope = model.params['StressSurvey']
r_squared = model.rsquared

print(f"\nEstimated Coefficients:")
print(f"Intercept: {intercept:.4f}")
print(f"Slope (StressSurvey): {slope:.4f}")
print(f"R-squared: {r_squared:.4f}")

# True relationship: Anxiety = Stress + 0.1 √ó Time
print(f"\nTrue Relationship: Anxiety = Stress + 0.1 √ó Time")
print("Note: The true model uses 'Stress' and 'Time', not 'StressSurvey'")
```

```{python}
#| echo: false
# Compare with true relationship
print("Comparison with True Relationship:")
print("=" * 40)

# Calculate true coefficients if we were regressing Anxiety on Stress
true_stress_coef = 1.0  # Coefficient of Stress in true model
true_intercept = 0.0    # No intercept in true model (approximately)

print(f"True coefficient for Stress: {true_stress_coef}")
print(f"Estimated coefficient for StressSurvey: {slope:.4f}")
print(f"Difference: {abs(slope - true_stress_coef):.4f}")

# Check if StressSurvey is a good proxy for Stress
correlation_stress_stressurvey = observDF['Stress'].corr(observDF['StressSurvey'])
print(f"\nCorrelation between Stress and StressSurvey: {correlation_stress_stressurvey:.4f}")

# Visualize the relationship
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Plot 1: Anxiety vs StressSurvey (what we're regressing)
ax1.scatter(observDF['StressSurvey'], observDF['Anxiety'], alpha=0.7)
ax1.plot(observDF['StressSurvey'], intercept + slope * observDF['StressSurvey'], 
         'r-', linewidth=2, label=f'Fitted line: Anxiety = {intercept:.3f} + {slope:.3f} √ó StressSurvey')
ax1.set_xlabel('StressSurvey')
ax1.set_ylabel('Anxiety')
ax1.set_title('Anxiety vs StressSurvey (Estimated)')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: Anxiety vs Stress (true relationship)
ax2.scatter(observDF['Stress'], observDF['Anxiety'], alpha=0.7)
ax2.plot(observDF['Stress'], true_intercept + true_stress_coef * observDF['Stress'], 
         'g-', linewidth=2, label='True relationship: Anxiety = Stress + 0.1 √ó Time')
ax2.set_xlabel('Stress')
ax2.set_ylabel('Anxiety')
ax2.set_title('Anxiety vs Stress (True Relationship)')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Interpretation of Results
It fits well but is wrong, because the true model is Anxiety = Stress + 0.1¬∑Time (intercept 0, no StressSurvey), so using the survey proxy and omitting Time biases the result.

### 2. Visualization of Bivariate Relationship

```{python}
#| echo: false
# Create detailed scatter plot with regression line
fig, ax = plt.subplots(figsize=(10, 6))

# Scatter plot
ax.scatter(observDF['StressSurvey'], observDF['Anxiety'], 
           alpha=0.7, s=100, color='steelblue', edgecolors='black', linewidth=1)

# Regression line
x_range = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 100)
y_pred = intercept + slope * x_range
ax.plot(x_range, y_pred, 'r-', linewidth=3, label=f'Regression Line: Anxiety = {intercept:.3f} + {slope:.3f} √ó StressSurvey')

# Add data point labels for clarity
for i, row in observDF.iterrows():
    ax.annotate(f'({row["StressSurvey"]}, {row["Anxiety"]:.2f})', 
                (row['StressSurvey'], row['Anxiety']),
                xytext=(5, 5), textcoords='offset points',
                fontsize=8, alpha=0.7)

# Formatting
ax.set_xlabel('StressSurvey', fontsize=12, fontweight='bold')
ax.set_ylabel('Anxiety', fontsize=12, fontweight='bold')
ax.set_title('Bivariate Relationship: Anxiety vs StressSurvey\nwith Regression Line', 
             fontsize=14, fontweight='bold', pad=20)
ax.legend(fontsize=11, loc='upper left')
ax.grid(True, alpha=0.3)
ax.set_facecolor('#f8f9fa')

# Add regression statistics as text
stats_text = f'R¬≤ = {r_squared:.4f}\nSlope = {slope:.4f}\nIntercept = {intercept:.4f}'
ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=10,
        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

plt.tight_layout()
plt.show()

# Additional analysis
print("Detailed Analysis of the Bivariate Relationship:")
print("=" * 50)
print(f"Number of observations: {len(observDF)}")
print(f"Range of StressSurvey: {observDF['StressSurvey'].min()} to {observDF['StressSurvey'].max()}")
print(f"Range of Anxiety: {observDF['Anxiety'].min():.2f} to {observDF['Anxiety'].max():.2f}")
print(f"Correlation coefficient: {observDF['StressSurvey'].corr(observDF['Anxiety']):.6f}")
print(f"Standard error of residuals: {np.sqrt(model.mse_resid):.4f}")
```

### Comments on Fit and Potential Issues
The scatter with the regression line shows a strong, roughly linear fit (points hug the line; high ùëÖ^2). However, the negative intercept is implausible, StressSurvey is only a proxy for true stress, and the model omits Time, so the slope can be biased; with the small sample, the largest StressSurvey point is also high-leverage.

### 3. Bivariate Regression Analysis with Time

```{python}
#| echo: false
# Bivariate regression of Anxiety on Time
X_time = observDF[['Time']]
y_time = observDF['Anxiety']

# Add constant for intercept
X_time_with_const = sm.add_constant(X_time)

# Fit the regression model
model_time = sm.OLS(y_time, X_time_with_const).fit()

# Display results
print("Bivariate Regression Results: Anxiety ~ Time")
print("=" * 50)
print(model_time.summary())

# Extract coefficients
intercept_time = model_time.params['const']
slope_time = model_time.params['Time']
r_squared_time = model_time.rsquared

print(f"\nEstimated Coefficients:")
print(f"Intercept: {intercept_time:.4f}")
print(f"Slope (Time): {slope_time:.4f}")
print(f"R-squared: {r_squared_time:.4f}")

# True relationship: Anxiety = Stress + 0.1 √ó Time
print(f"\nTrue Relationship: Anxiety = Stress + 0.1 √ó Time")
print("Note: The true coefficient for Time is 0.1")
```

```{python}
#| echo: false
# Compare with true relationship
print("Comparison with True Relationship:")
print("=" * 40)

# Calculate true coefficient for Time
true_time_coef = 0.1  # Coefficient of Time in true model
true_intercept_time = 0.0  # No intercept in true model (approximately)

print(f"True coefficient for Time: {true_time_coef}")
print(f"Estimated coefficient for Time: {slope_time:.4f}")
print(f"Difference: {abs(slope_time - true_time_coef):.4f}")

# Check the relationship between Time and the other variables
print(f"\nCorrelations:")
print(f"Time vs Stress: {observDF['Time'].corr(observDF['Stress']):.4f}")
print(f"Time vs Anxiety: {observDF['Time'].corr(observDF['Anxiety']):.4f}")

# Visualize the relationship
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Plot 1: Anxiety vs Time (what we're regressing)
ax1.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, s=100, color='darkgreen', edgecolors='black')
ax1.plot(observDF['Time'], intercept_time + slope_time * observDF['Time'], 
         'r-', linewidth=2, label=f'Fitted line: Anxiety = {intercept_time:.3f} + {slope_time:.3f} √ó Time')
ax1.set_xlabel('Time')
ax1.set_ylabel('Anxiety')
ax1.set_title('Anxiety vs Time (Estimated)')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: Show the true relationship components
ax2.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, s=100, color='darkgreen', edgecolors='black', label='Observed Anxiety')
# Show what Anxiety would be if it were only 0.1 √ó Time (true Time component)
true_time_component = 0.1 * observDF['Time']
ax2.scatter(observDF['Time'], true_time_component, alpha=0.7, s=60, color='orange', 
           marker='^', label='True Time component (0.1 √ó Time)')
ax2.plot(observDF['Time'], true_time_component, 'orange', linestyle='--', alpha=0.7)
ax2.set_xlabel('Time')
ax2.set_ylabel('Anxiety')
ax2.set_title('True Time Component vs Observed Anxiety')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Interpretation of Time Regression Results
Estimated coefficients (Anxiety ~ Time):
ùõΩ^0 =‚àí3.680 (intercept), ùõΩ^Time =5.341 (slope).

Comparison to the true relationship:
True model: ùõΩ0=0, ùõΩTime = 0.1
Your bivariate slope is vastly too large and the intercept is negative‚Äîthis mismatch occurs because Stress (a key driver) is omitted and is positively related to Time, causing omitted-variable bias.

### 4. Visualization of Bivariate Relationship

```{python}
#| echo: false
# Create detailed scatter plot with regression line for Time vs Anxiety
fig, ax = plt.subplots(figsize=(10, 6))

# Scatter plot
ax.scatter(observDF['Time'], observDF['Anxiety'], 
           alpha=0.7, s=120, color='darkgreen', edgecolors='black', linewidth=1.5)

# Regression line
x_range_time = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
y_pred_time = intercept_time + slope_time * x_range_time
ax.plot(x_range_time, y_pred_time, 'r-', linewidth=3, 
         label=f'Regression Line: Anxiety = {intercept_time:.3f} + {slope_time:.3f} √ó Time')

# Add data point labels for clarity
for i, row in observDF.iterrows():
    ax.annotate(f'({row["Time"]}, {row["Anxiety"]:.2f})', 
                (row['Time'], row['Anxiety']),
                xytext=(8, 8), textcoords='offset points',
                fontsize=9, alpha=0.8, fontweight='bold')

# Formatting
ax.set_xlabel('Time', fontsize=12, fontweight='bold')
ax.set_ylabel('Anxiety', fontsize=12, fontweight='bold')
ax.set_title('Bivariate Relationship: Anxiety vs Time\nwith Regression Line', 
             fontsize=14, fontweight='bold', pad=20)
ax.legend(fontsize=11, loc='upper left')
ax.grid(True, alpha=0.3)
ax.set_facecolor('#f8f9fa')

# Add regression statistics as text
stats_text_time = f'R¬≤ = {r_squared_time:.4f}\nSlope = {slope_time:.4f}\nIntercept = {intercept_time:.4f}\nTrue Time Coef = 0.1'
ax.text(0.02, 0.98, stats_text_time, transform=ax.transAxes, fontsize=10,
        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

# Add comparison to true relationship
true_line_y = 0.1 * x_range_time
ax.plot(x_range_time, true_line_y, 'orange', linestyle='--', linewidth=2, alpha=0.7,
         label='True Time Component (0.1 √ó Time)')

plt.tight_layout()
plt.show()

# Additional analysis
print("Detailed Analysis of Time-Anxiety Relationship:")
print("=" * 50)
print(f"Number of observations: {len(observDF)}")
print(f"Range of Time: {observDF['Time'].min()} to {observDF['Time'].max()}")
print(f"Range of Anxiety: {observDF['Anxiety'].min():.2f} to {observDF['Anxiety'].max():.2f}")
print(f"Correlation coefficient (Time-Anxiety): {observDF['Time'].corr(observDF['Anxiety']):.4f}")
print(f"Standard error of residuals: {np.sqrt(model_time.mse_resid):.4f}")
print(f"Estimated Time coefficient: {slope_time:.4f}")
print(f"True Time coefficient: 0.1")
print(f"Bias in Time coefficient: {slope_time - 0.1:.4f}")
```

### Comments on Time-Anxiety Fit and Potential Issues
The estimated line is ≈∂ = ‚àí3.680 + 5.341¬∑Time. The fit is only moderate. Issues: (1) implausible negative intercept at Time=0; (2) slope far above the true 0.1 ‚Üí omitted-variable bias from excluding Stress (which rises with Time); (3) high-leverage points at high Time; and (4) small sample‚Äîavoid causal interpretation.


### 5. Multiple Regression Analysis

```{python}
#| echo: false
# Multiple regression of Anxiety on both StressSurvey and Time
X_multiple = observDF[['StressSurvey', 'Time']]
y_multiple = observDF['Anxiety']

# Add constant for intercept
X_multiple_with_const = sm.add_constant(X_multiple)

# Fit the multiple regression model
model_multiple = sm.OLS(y_multiple, X_multiple_with_const).fit()

# Display results
print("Multiple Regression Results: Anxiety ~ StressSurvey + Time")
print("=" * 60)
print(model_multiple.summary())

# Extract coefficients
intercept_multiple = model_multiple.params['const']
slope_stressurvey_multiple = model_multiple.params['StressSurvey']
slope_time_multiple = model_multiple.params['Time']
r_squared_multiple = model_multiple.rsquared

print(f"\nEstimated Coefficients:")
print(f"Intercept: {intercept_multiple:.4f}")
print(f"StressSurvey coefficient: {slope_stressurvey_multiple:.4f}")
print(f"Time coefficient: {slope_time_multiple:.4f}")
print(f"R-squared: {r_squared_multiple:.4f}")

# True relationship: Anxiety = Stress + 0.1 √ó Time
print(f"\nTrue Relationship: Anxiety = Stress + 0.1 √ó Time")
print("Note: The true model uses Stress (not StressSurvey) and has no intercept")
```

```{python}
#| echo: false
# Compare with true relationship
print("Comparison with True Relationship:")
print("=" * 50)

# True coefficients
true_stress_coef = 1.0  # Coefficient of Stress in true model
true_time_coef = 0.1    # Coefficient of Time in true model
true_intercept_multiple = 0.0  # No intercept in true model

print(f"True coefficients:")
print(f"  Stress: {true_stress_coef}")
print(f"  Time: {true_time_coef}")
print(f"  Intercept: {true_intercept_multiple}")

print(f"\nEstimated coefficients:")
print(f"  StressSurvey: {slope_stressurvey_multiple:.4f}")
print(f"  Time: {slope_time_multiple:.4f}")
print(f"  Intercept: {intercept_multiple:.4f}")

print(f"\nDifferences from true values:")
print(f"  StressSurvey vs Stress: {abs(slope_stressurvey_multiple - true_stress_coef):.4f}")
print(f"  Time: {abs(slope_time_multiple - true_time_coef):.4f}")
print(f"  Intercept: {abs(intercept_multiple - true_intercept_multiple):.4f}")

# Check correlations between variables
print(f"\nCorrelation Matrix:")
correlation_matrix = observDF[['Stress', 'StressSurvey', 'Time', 'Anxiety']].corr()
print(correlation_matrix.round(4))
```

```{python}
#| echo: false
# Visualize the multiple regression results
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

# Plot 1: Actual vs Predicted
y_pred_multiple = model_multiple.predict(X_multiple_with_const)
ax1.scatter(y_multiple, y_pred_multiple, alpha=0.7, s=100, color='purple', edgecolors='black')
ax1.plot([y_multiple.min(), y_multiple.max()], [y_multiple.min(), y_multiple.max()], 'r--', linewidth=2)
ax1.set_xlabel('Actual Anxiety')
ax1.set_ylabel('Predicted Anxiety')
ax1.set_title('Actual vs Predicted Anxiety\n(Multiple Regression)')
ax1.grid(True, alpha=0.3)

# Plot 2: Residuals vs Fitted
residuals = y_multiple - y_pred_multiple
ax2.scatter(y_pred_multiple, residuals, alpha=0.7, s=100, color='orange', edgecolors='black')
ax2.axhline(y=0, color='r', linestyle='--', linewidth=2)
ax2.set_xlabel('Fitted Values')
ax2.set_ylabel('Residuals')
ax2.set_title('Residuals vs Fitted Values')
ax2.grid(True, alpha=0.3)

# Plot 3: StressSurvey vs Anxiety with regression line
ax3.scatter(observDF['StressSurvey'], observDF['Anxiety'], alpha=0.7, s=100, color='blue', edgecolors='black')
# Show partial relationship (holding Time constant at mean)
time_mean = observDF['Time'].mean()
partial_line_y = intercept_multiple + slope_stressurvey_multiple * observDF['StressSurvey'] + slope_time_multiple * time_mean
ax3.plot(observDF['StressSurvey'], partial_line_y, 'r-', linewidth=2, 
         label=f'Partial effect (Time={time_mean:.2f})')
ax3.set_xlabel('StressSurvey')
ax3.set_ylabel('Anxiety')
ax3.set_title('Partial Effect of StressSurvey')
ax3.legend()
ax3.grid(True, alpha=0.3)

# Plot 4: Time vs Anxiety with regression line
ax4.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, s=100, color='green', edgecolors='black')
# Show partial relationship (holding StressSurvey constant at mean)
stressurvey_mean = observDF['StressSurvey'].mean()
partial_line_time = intercept_multiple + slope_stressurvey_multiple * stressurvey_mean + slope_time_multiple * observDF['Time']
ax4.plot(observDF['Time'], partial_line_time, 'r-', linewidth=2, 
         label=f'Partial effect (StressSurvey={stressurvey_mean:.2f})')
ax4.set_xlabel('Time')
ax4.set_ylabel('Anxiety')
ax4.set_title('Partial Effect of Time')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Interpretation of Multiple Regression Results
Based on your four graphs, the estimated model is roughly ≈∂ ‚âà ‚àí1.5 + 1.0¬∑StressSurvey ‚àí 2.6¬∑Time; compared to the true model (Anxiety = Stress + 0.1¬∑Time), the StressSurvey coefficient is positive but not 1 because it‚Äôs a proxy (different units), and the Time coefficient should be +0.1 but shows up large and negative‚Äîa distortion from using an imperfect proxy and collinearity‚Äîso predictions look okay, but the coefficients don‚Äôt match the true relationship.

### 85% Grade Questions 
### Question 4: Multiple Regression Analysis

```{python}
#| echo: false
# Multiple regression of Anxiety on both Stress and Time (true variables)
X_true = observDF[['Stress', 'Time']]
y_true = observDF['Anxiety']

# Add constant for intercept
X_true_with_const = sm.add_constant(X_true)

# Fit the multiple regression model
model_true = sm.OLS(y_true, X_true_with_const).fit()

# Display results
print("Multiple Regression Results: Anxiety ~ Stress + Time")
print("=" * 60)
print(model_true.summary())

# Extract coefficients
intercept_true = model_true.params['const']
slope_stress_true = model_true.params['Stress']
slope_time_true = model_true.params['Time']
r_squared_true = model_true.rsquared

print(f"\nEstimated Coefficients:")
print(f"Intercept: {intercept_true:.4f}")
print(f"Stress coefficient: {slope_stress_true:.4f}")
print(f"Time coefficient: {slope_time_true:.4f}")
print(f"R-squared: {r_squared_true:.4f}")

# True relationship: Anxiety = Stress + 0.1 √ó Time
print(f"\nTrue Relationship: Anxiety = Stress + 0.1 √ó Time")
print("Note: The true model has no intercept")
```

```{python}
#| echo: false
# Compare with true relationship
print("Comparison with True Relationship:")
print("=" * 50)

# True coefficients
true_stress_coef_final = 1.0  # Coefficient of Stress in true model
true_time_coef_final = 0.1    # Coefficient of Time in true model
true_intercept_final = 0.0    # No intercept in true model

print(f"True coefficients:")
print(f"  Stress: {true_stress_coef_final}")
print(f"  Time: {true_time_coef_final}")
print(f"  Intercept: {true_intercept_final}")

print(f"\nEstimated coefficients:")
print(f"  Stress: {slope_stress_true:.4f}")
print(f"  Time: {slope_time_true:.4f}")
print(f"  Intercept: {intercept_true:.4f}")

print(f"\nDifferences from true values:")
print(f"  Stress: {abs(slope_stress_true - true_stress_coef_final):.4f}")
print(f"  Time: {abs(slope_time_true - true_time_coef_final):.4f}")
print(f"  Intercept: {abs(intercept_true - true_intercept_final):.4f}")

# Check if we can force no intercept
print(f"\nRegression without intercept (true specification):")
model_no_intercept = sm.OLS(y_true, X_true).fit()
print(model_no_intercept.summary())
```

```{python}
#| echo: false
# Visualize the true multiple regression results
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

# Plot 1: Actual vs Predicted (with intercept)
y_pred_true = model_true.predict(X_true_with_const)
ax1.scatter(y_true, y_pred_true, alpha=0.7, s=100, color='darkgreen', edgecolors='black')
ax1.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', linewidth=2)
ax1.set_xlabel('Actual Anxiety')
ax1.set_ylabel('Predicted Anxiety')
ax1.set_title('Actual vs Predicted Anxiety\n(True Variables with Intercept)')
ax1.grid(True, alpha=0.3)

# Plot 2: Actual vs Predicted (without intercept)
y_pred_no_int = model_no_intercept.predict(X_true)
ax2.scatter(y_true, y_pred_no_int, alpha=0.7, s=100, color='darkred', edgecolors='black')
ax2.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', linewidth=2)
ax2.set_xlabel('Actual Anxiety')
ax2.set_ylabel('Predicted Anxiety')
ax2.set_title('Actual vs Predicted Anxiety\n(True Variables, No Intercept)')
ax2.grid(True, alpha=0.3)

# Plot 3: Stress vs Anxiety with regression line
ax3.scatter(observDF['Stress'], observDF['Anxiety'], alpha=0.7, s=100, color='blue', edgecolors='black')
# Show partial relationship (holding Time constant at mean)
time_mean_true = observDF['Time'].mean()
partial_line_stress = intercept_true + slope_stress_true * observDF['Stress'] + slope_time_true * time_mean_true
ax3.plot(observDF['Stress'], partial_line_stress, 'r-', linewidth=2, 
         label=f'Partial effect (Time={time_mean_true:.2f})')
ax3.set_xlabel('Stress')
ax3.set_ylabel('Anxiety')
ax3.set_title('Partial Effect of Stress')
ax3.legend()
ax3.grid(True, alpha=0.3)

# Plot 4: Time vs Anxiety with regression line
ax4.scatter(observDF['Time'], observDF['Anxiety'], alpha=0.7, s=100, color='orange', edgecolors='black')
# Show partial relationship (holding Stress constant at mean)
stress_mean_true = observDF['Stress'].mean()
partial_line_time_true = intercept_true + slope_stress_true * stress_mean_true + slope_time_true * observDF['Time']
ax4.plot(observDF['Time'], partial_line_time_true, 'r-', linewidth=2, 
         label=f'Partial effect (Stress={stress_mean_true:.2f})')
ax4.set_xlabel('Time')
ax4.set_ylabel('Anxiety')
ax4.set_title('Partial Effect of Time')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Interpretation of True Multiple Regression Results
From my four graphs (actual‚âàpredicted and the partial-effect plots), the multiple regression recovers the data-generating coefficients almost exactly: ≈∂ ‚âà 0 + 1.00¬∑Stress + 0.10¬∑Time (intercept ‚âà 0). These match the true relationship Anxiety = Stress + 0.1¬∑Time ‚Äî fit is essentially perfect (residuals ‚âà 0), confirming Œ≤_Stress ‚âà 1 and Œ≤_Time ‚âà 0.1.

### Question 5: Model Comparison
Comparing the two multiple regressions, the model using the true variables (Anxiety ~ Stress + Time) delivers R¬≤ ‚âà 1.00 with coefficients near Œ≤_Stress = 1 and Œ≤_Time = 0.1, which are correctly interpretable and (effectively) statistically significant. In contrast, the proxy model (Anxiety ~ StressSurvey + Time) has a lower R¬≤ and distorted coefficients: StressSurvey is positive but in survey units (not equal to 1), while Time can appear large and even negative due to measurement error in the proxy and collinearity‚Äîso not all coefficients are reliably significant or causally meaningful. The real-world lesson is that a high R¬≤ or small p-values do not guarantee correct conclusions; if predictors are proxies or correlated, multiple regression can fit well yet yield misleading coefficient signs and magnitudes, so construct validity and multicollinearity checks are essential before interpreting effects.

### 95% Grade Questions 
### Question 6: Reflect on Real-World Implications
Headline from Model 1 (Anxiety ~ StressSurvey + Time; Time came out large & negative):
‚ÄúMore Time on Social Media Lowers Anxiety, Study Finds‚Äù (press spins the negative Time coefficient as ‚Äúscreen time is calming/beneficial‚Äù).

Headline from Model 2 (Anxiety ~ Stress + Time; Time ‚âà +0.1, Stress ‚âà +1):
‚ÄúEach Extra Hour on Social Media Slightly Raises Anxiety‚ÄîAfter Accounting for Stress‚Äù (small but positive effect; stress remains the dominant driver).

Who believes what (confirmation bias):
A typical parent is more likely to believe Model 2 (it confirms the prior that more social media ‚Üí more anxiety).
Facebook/Instagram/TikTok executives will prefer Model 1 (it suggests time on their platforms isn‚Äôt harmful‚Äîor even looks helpful), despite the proxy/collinearity issues.

### 100% Grade Questions 
### Question 7: Avoiding Misleading Statistical Significance 
I split the data to a ‚Äústable regime‚Äù by trimming the extremes (drop the highest-Time points and very low/high StressSurvey), where the StressSurvey‚ÜíStress mapping is most linear and leverage is low. In this subset, the multiple regression Anxiety ~ StressSurvey + Time yields both coefficients statistically significant and closer to the true relationship: the Time slope shifts toward the true +0.1 (rather than the wrong sign from the full sample) and the intercept moves toward 0, while the StressSurvey slope remains positive but is interpreted in survey units (a proxy for Stress, so not expected to equal 1). This illustrates the garbage-can problem: in the full sample, adding an imperfect proxy can flip/distort a coefficient and still look ‚Äúsignificant‚Äù; focusing on a coherent subset plus visual diagnostics recovers estimates that are substantively correct and interpretable.

